1.Apache Spark was developed in order to provide solutions to shortcomings of another project, and eventually replace it. What is the name of this project?

- MapReduce

2.Why is Hadoop MapReduce slow for iterative algorithms?

- It needs to read off disk for every iteration

3.What is the most important feature of Apache Spark to speedup iterative algorithms?

- Caching datasets in memory

4.Which other Hadoop project can Spark rely to provision and manage the cluster of nodes?

- YARN

5.When Spark reads data out of HDFS, what is the process that interfaces directly with HDFS?

- Executor

6.Under which circumstances is preferable to run Spark in Standalone mode instead of relying on YARN?

- When you only plan on running Spark jobs



